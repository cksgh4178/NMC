# 인공 신경망

선형으로 표현할 수 없는 2차 함수의 형태를 가지는 데이터에 대한 신경망 모델

```python
import torch
import torch.nn as nn
import pandas as pd

# data
x = nn.init.uniform_(torch.Tensor(100, 1), -15, 15)
noise = nn.init.normal_(torch.Tensor(100,1), std = 1)
y = x**2 + 3 + noise

# plotting
plot = pd.DataFrame(columns=['x', 'y'], data = torch.cat((x,y),dim=1).numpy())
plot.sort_values(by = 'x', inplace = True)
plot.plot.scatter(x = 'x', y = 'y')
```

* x는 [-15, 15] 사이에 균등하게 분포
* y는 x^2 + 3에 노이즈가 추가

![Figure](https://user-images.githubusercontent.com/23060537/142360851-53de384d-29db-4039-9392-46c465f5a6bc.png)

모델 구성(보통 모델만 다른 모듈에 따로 구성함)

```python
model = nn.Sequential(
    nn.Linear(1,6),
    nn.ReLU(),
    nn.Linear(6, 10),
    nn.ReReLU(),
    nn.Linear(10, 6),
    nn.ReLU(),
    nn.Linear(6, 1))
```
*  input (1차원) -> 은닉층 1 (6차원)
* 은닉층1(6차원) -> 은닉층 2(10차원)
* 은닉층2(10차원) -> 은닉층 3(6차원)
* 은닉층3(6차원) -> 출력(1차원)

학습
```python
# 학습 구성 및 추적
import wandb
wandb.init(project='pytorch_start', entity='cksgh4178')
wandb.run.name = 'ann'

epoch = 500
learning_rate = 0.0001

criterion = nn.L1Loss()
optimizer = torch.optim.SGD(model.parameters(), lr= learning_rate)

model.train()
for i in range(epoch):
    pred = model(x)
    optimizer.zero_grad()
    loss = criterion(pred, y)
    loss.backward()
    optimizer.step()
    
    wandb.log({
        'loss': loss.item()})

# 결과 비교
plot = pd.DataFrame(columns=['x', 'real','pred'], data = torch.cat((x,y, pred.detach()),dim=1).numpy())
plot.sort_values(by = 'x', inplace = True)
plot.plot(x = 'x', ylabel = 'y')
```
<img src="https://user-images.githubusercontent.com/23060537/142360906-77d534f7-973f-486b-a3f6-ad9490e55a21.png" width="500" height="300">

<img src="https://user-images.githubusercontent.com/23060537/142361351-aa342fb4-eeeb-486e-a1ae-4deaf378d1dd.png" width="500" height="300">

옵티마이저와 학습율에 따라 실제값-예측값 그림 많이 달라짐

옵티마이저의 영향과 y가 [0,1] 범위가 아니어서 그런 것도 있음
