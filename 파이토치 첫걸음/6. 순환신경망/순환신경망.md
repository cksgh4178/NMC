# 순환신경망
* Recurrent Neural Network
	* 순환 신경망은 시간에 따라 변화하는 시계열 데이터를 처리하기 위한 네트워크 구조
* 순환 신경망의 구조
<center>
<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSU0Pk_u3VJgFm4Xpuu7izmojZjoD8DMPiRNg&usqp=CAU" width="600" height = "250">
<img src="https://t2.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/17Xk/image/CaG8aFbUMK9sF6ce-JQewa125sE.png" width = "600" height="250">
</center>
* t = 0 에서 첫 번째 은닉층 값이 계산되면 t = 1에서 그 출력값이 다시 해당 은닉층으로 들어가는 구조
* 이런 구조이기 때문에 이전의 입력값을 고려할 수 있게 됨
* 역전파 과정에서 **시간에 따른 역전파(Backpropagation though time; BPTT)** 를 사용
	* 은닉층의 계산이 사용된 시점의 수에 영향을 받음
	* t = 0에서 원하는 결과값은 t = 1에서의 input
	* 시간을 역으로 거슬러 올라가는 방식으로 각 가중치들을 업데이트해야함

#### utils.py

```python
import torch
import numpy as np

def string_to_onehot(string, shape, char_list):
    start = np.zeros(shape=shape, dtype=int)
    end = np.zeros(shape=shape, dtype=int)
    start[-2] = 1
    end[-1] = 1
    for i in string:
        idx = char_list.index(i)
        zero = np.zeros(shape=shape, dtype=int)
        zero[idx] = 1
        start = np.vstack([start, zero])
    output = np.vstack([start, end])
    return output

def onehot_to_word(vector, char_list):
    onehot = torch.Tensor.numpy(vector)
    return char_list[onehot.argmax()]
```

#### RNN.py

```python
import torch
import torch.nn as nn

class rnn(nn.Module):
    
    def __init__(self, input_size, hidden_size, output_size):
        super(rnn, self).__init__()
        
        self.input_size = input_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        
        # RNN cell 직접 구성
        self.i2h = nn.Linear(input_size, hidden_size)
        self.h2h = nn.Linear(hidden_size, hidden_size)
        self.h2o = nn.Linear(hidden_size, output_size)
        self.act_fn = nn.Tanh()
        
    def forward(self, input, hidden):
        hidden = self.act_fn(self.i2h(input)+self.h2h(hidden))
        output = self.h2o(hidden)
        return output, hidden
    
    def init_hidden(self):
        return torch.zeros(1, self.hidden_size)
```

#### launch.py

```python
import os
import sys
import torch
import torch.nn as nn
import wandb

base_dir = os.path.dirname(os.getcwd())
sys.path.append(base_dir)

from src.utils import string_to_onehot, onehot_to_word
from src.RNN import rnn

# wandb setting
wandb.init(project = '...', entity='...')
wandb.run.name = 'rnn'

# data
string = 'hello pytorch. how long can a rnn cell remember?'
chars = 'abcdefghijklmnopqrstuvwxyz ?!.,:;01' # 마지막 두 개가 start, end 표지
char_list = [i for i in chars]
n_letters = len(char_list)

# settings
n_hidden = 35
lr = 0.01
epochs = 1000

model = rnn(n_letters, n_hidden, n_letters)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = lr)

# train
onehot = torch.from_numpy(string_to_onehot(string, n_letters, char_list)).type_as(torch.FloatTensor())

for i in range(epochs):
    optimizer.zero_grad()
    total_loss = 0
    hidden = model.init_hidden()
    
    for j in range(onehot.size(0)-1):
        input_ = onehot[j:j+1,:]
        target = onehot[j+1]
        
        output, hidden = model(input_, hidden)
        loss = criterion(output.view(-1), target.view(-1))
        total_loss += loss
        input_ = output
   
    total_loss.backward()
    optimizer.step()
    
    wandb.log({'loss':total_loss.item()})
    
# test
start = torch.zeros(1, n_letters)
start[:, -2] = 1

with torch.no_grad():
    hidden = model.init_hidden()
    input_ = start
    output_string = ''
    for i in range(len(string)):
        output, hidden = model(input_, hidden)
        output_string += onehot_to_word(output.data, char_list)
        input_ = output

print(output_string)
```

* 실험 결과
<img src="https://user-images.githubusercontent.com/23060537/142850339-9f5d2913-215e-46be-bda8-1afa14fdd7f6.png" width="500" height="300">

* 기존 순환신경망은 입력 길이가 길어질수록 학습이 잘되지 않는 *기울기 소실(vanishing gradient)* 문제가 발생
	* LSTM, GRU 같은 새로운 구조를 사용해 이 문제를 해결

* 임베딩(Embedding)
  * one-hot 인코딩의 문제점
    * 원-핫 벡터 간의 내적은 항상 0 이므로 유사도나 차이 계산 불가
    * 벡터의 길이에 비해 의미없는 정보가 너무 많음 (단어 수가 늘어날수록 벡터 길이도 함께 늘어남)
  * 임베딩은 알파벳이나 단어같은 요소들을 일정한 길이를 가진 벡터 공간에 투영하는 것 (단어를 벡터화하는 것을 **word2vec**이라고 함)
    * CBOW : 주변 단어들로부터 중심 단어가 나오도록 임베딩하는 방식
    * skip-gram : 중심 단어로부터 주변 단어들이 나오도록 임베딩하는 방식

#### 여러가지 모델 비교
#### RNN.py (nn.RNN 사용)

```python
import torch
import torch.nn as nn

class rnn(nn.Module):
    
    def __init__(self, input_size, embedding_size, hidden_size, output_size, batch_size, num_layers=1):
        super(rnn, self).__init__()
        self.input_size = input_size
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.encoder = nn.Embedding(input_size, embedding_size)
        self.rnn = nn.RNN(embedding_size, hidden_size, num_layers)
        self.decoder = nn.Linear(hidden_size, output_size)
        
    def forward(self, input, hidden):
        out = self.encoder(input.view(-1, 1))
        out, hidden = self.rnn(out)
        out = self.decoder(out.view(self.batch_size, -1))
        return out, hidden
    
    def init_hidden(self):
        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
        return hidden
```

#### GRU.py

```python
import torch
import torch.nn as nn

class gru(nn.Module):
    
    def __init__(self, input_size, embedding_size, hidden_size, output_size, batch_size, num_layers=1):
        super(gru, self).__init__()
        self.input_size = input_size
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.encoder = nn.Embedding(input_size, embedding_size)
        self.gru = nn.GRU(embedding_size, hidden_size, num_layers)
        self.decoder = nn.Linear(hidden_size, output_size)
        
    def forward(self, input, hidden):
        out = self.encoder(input.view(-1, 1))
        out, hidden = self.gru(out)
        out = self.decoder(out.view(self.batch_size, -1))
        return out, hidden
    
    def init_hidden(self):
        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
        return hidden
```

#### LSTM.py

```python
import torch
import torch.nn as nn

class lstm(nn.Module):
    
    def __init__(self, input_size, embedding_size, hidden_size, output_size, batch_size, num_layers=1):
        super(lstm, self).__init__()
        self.input_size = input_size
        self.embedding_size = embedding_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.num_layers = num_layers
        self.batch_size = batch_size
        self.encoder = nn.Embedding(input_size, embedding_size)
        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers)
        self.decoder = nn.Linear(hidden_size, output_size)
        
    def forward(self, input, hidden, cell):
        out = self.encoder(input.view(-1, 1))
        out, (hidden, cell) = self.lstm(out,(hidden, cell))
        out = self.decoder(out.view(self.batch_size, -1))
        return out, hidden, cell
    
    def init_hidden(self):
        hidden = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
        cell = torch.zeros(self.num_layers, self.batch_size, self.hidden_size)
        return hidden, cell
```

여러가지 모델 비교는 책의 [코랩 사이트](https://drive.google.com/drive/folders/1aUUhFYKF4rHqhxxPDz3xfRwxgLatqcQF)에서 확인
