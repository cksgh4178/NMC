# 오토인코더
<br>

## 기본적인 오토인코더
<br>

* 데이터에 대한 효율적인 압축을 신경망을 통해 자동으로 학습하는 모델
* 여러 가지 오토 인코더 그림

![그림](https://user-images.githubusercontent.com/23060537/143793975-bcb94cfa-afbe-4cc5-88ff-8936adcd81d7.png)

* 일반적으로 입력 데이터 자체가 라벨로 사용되기 때문에 비지도 학습에 속함
	* 입력 차원보다 낮은 차원으로 압축되기 때문에 *인코딩(encoding)*, *특성 학습(feature learning)*, *표현 학습(representation learning)* 등으로 불림
* 기본적인 오토 인코더 형태

![그림](https://t1.daumcdn.net/thumb/R720x0/?fname=http://t1.daumcdn.net/brunch/service/user/16xs/image/7QkEB4iJP9IcuY9YvVaemG-jhIU.png)

* 여기서 가운데 code 부분을 잠재 변수(latent variable)로써 사용
* 예시 코드 (Mnist 데이터 사용)

#### model.py
```python
import torch
import torch.nn as nn

class Autoencoder(nn.Module):
    
    def __init__(self, batch_size):
        super(Autoencoder, self).__init__()
        self.batch_size = batch_size
        self.encoder = nn.Linear(28*28, 20)
        self.decoder = nn.Linear(20, 28*28)
        
    def forward(self, x):
        x = x.view(self.batch_size, -1)
        encoded = self.encoder(x)
        out = self.decoder(encoded).view(self.batch_size, 1, 28, 28)
        return out
```

#### main.py

```python
batch_size = 256
epochs = 200

model = Autoencoder(batch_size)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)

for i in range(epochs):
    for _, (image, label) in enumerate(train_loader):
        pred = model(image)
        optimizer.zero_grad()
        loss = criterion(pred, image)
        loss.backward()
        optimizer.step()
```

<br>
## 합성곱 오토인코더
<br>

* 이미지 데이터의 경우 기본적인 신경망을 쓰는 대신 conv layer를 쓰는 것이 더 효율적
	* 디코더 부분에서 인코더와 대칭되게 만들려면 **전치 합성곱(Transposed conv)** 혹은 **역합성곱** 연산을 하는 것이 핵심
* 전치 합성곱 : 하나의 입력값을 받아 필터 크기만큼 퍼트리는 역할
* 파이토치 상의 nn.ConvTransposed2d 함수 사용

#### model.py

```python
import torch
import torch.nn as nn

class encoder(nn.Module):
    
    def __init__(self):
        super(encoder, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, 3, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.Conv2d(16, 32, 3, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(32),
            nn.Conv2d(32, 64, 3, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(64),
            nn.MaxPool2d(2,2)
            )
        
        self.layer2 = nn.Sequential(
            nn.Conv2d(64, 128, 3, padding = 1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
            nn.MaxPool2d(2,2),
            nn.Conv2d(128, 256, 3, padding = 1),
            nn.ReLU()
            )

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(x)
        out = out.view(x.size(0), -1)
        return out

class decoder(nn.Module):
    
    def __init__(self):
        super(decoder, self).__init__()
        self.layer1 = nn.Sequential(
            nn.ConvTranspose2d(256, 128, 3, 2, 1, 1),
            nn.ReLU(),
            nn.BatchNorm2d(128),
            nn.ConvTranspose2d(128, 64, 3, 1, 1),
            nn.ReLU(),
            nn.BatchNorm2d(64)
            )
        self.layer2 = nn.Sequential(
            nn.ConvTranspose2d(64, 16, 3, 1, 1),
            nn.ReLU(),
            nn.BatchNorm2d(16),
            nn.ConvTranspose2d(16, 1, 3, 2, 1, 1),
            nn.ReLU()
            )
    
    def forward(self, x):
        out = x.view(x.size(0), 256, 7, 7)
        out = self.layer1(out)
        out = self.layer2(out)
        return out
```

* 학습 과정에서 encoder, decoder 모두를 업데이트하고 적절히 학습되었으면 encoder 부분만 따로 사용
<br>

## 시맨틱 세그먼테이션
<br>
* 이미지 간 이전(translation) 모델을 활용
* 생체 데이터 이미지에서 세포 내부를 구분하는 U-net이 대표적
	* 그 외에도 항공 사진을 지도로 변환하는 작업 등이 대표적

#### model.py

```python
import torch
import torch.nn as nn

def conv_block(in_dim, out_dim, act_fn):
    model = nn.Sequential(
        nn.Conv2d(in_dim, out_dim, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(out_dim),
        act_fn
        )
    return model

def conv_trans_block(in_dim, out_dim, act_fn):
    model = nn.Sequential(
        nn.ConvTranspose2d(in_dim, out_dim, kernel_size=3, stride=2, padding=1, output_padding=1),
        nn.BatchNorm2d(out_dim),
        act_fn
        )
    return model

def maxpool():
    pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)
    return pool

def conv_block_2(in_dim, out_dim, act_fn):
    model = nn.Sequential(
        conv_block(in_dim, out_dim, act_fn),
        nn.Conv2d(out_dim, out_dim, kernel_size=3, stride=1, padding=1),
        nn.BatchNorm2d(out_dim)
        )
    return model

class UnetGenerator(nn.Module):
    
    def __init__(self, in_dim, out_dim, num_filter):
        super(UnetGenerator, self).__init__()
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.num_filter = num_filter
        act_fn = nn.LeakyReLU(0.2, inplace=True)
        
        self.down_1 = conv_block_2(in_dim, num_filter, act_fn)
        self.pool_1 = maxpool()
        self.down_2 = conv_block_2(num_filter, num_filter*2, act_fn)
        self.pool_2 = maxpool()
        self.down_3 = conv_block_2(num_filter*2, num_filter*4, act_fn)
        self.pool_3 = maxpool()
        self.down_4 = conv_block_2(num_filter*4, num_filter*8, act_fn)
        self.pool_4 = maxpool()
        
        self.bridge = conv_block_2(num_filter*8, num_filter*16, act_fn)
        
        self.trans_1 = conv_trans_block(num_filter*16, num_filter*8, act_fn)
        self.up_1 = conv_block_2(num_filter*16, num_filter*8, act_fn)
        self.trans_2 = conv_trans_block(num_filter*8, num_filter*4, act_fn)
        self.up_2 = conv_block_2(num_filter*8, num_filter*4, act_fn)
        self.trans_3 = conv_trans_block(num_filter*4, num_filter*2, act_fn)
        self.up_3 = conv_block_2(num_filter*4, num_filter*2, act_fn)
        self.trans_4 = conv_trans_block(num_filter*2, num_filter, act_fn)
        self.up_4 = conv_block_2(num_filter*2, num_filter, act_fn)
        
        self.out = nn.Sequential(
            nn.Conv2d(num_filter, out_dim, kernel_size=3, stride=1, padding=1),
            nn.Tanh()
            )
        
    def forward(self, input):
        down_1 = self.down_1(input)
        pool_1 = self.pool_1(down_1)
        down_2 = self.down_2(pool_1)
        pool_2 = self.pool_2(down_2)
        down_3 = self.down_3(pool_2)
        pool_3 = self.pool_3(down_3)
        down_4 = self.down_4(pool_3)
        pool_4 = self.pool_4(down_4)
        
        bridge = self.bridge(pool_4)
        
        trans_1 = self.trans_1(bridge)
        concat_1 = torch.cat([trans_1, down_4], dim=1)
        up_1 = self.up_1(concat_1)
        trans_2 = self.trans_2(up_1)
        concat_2 = torch.cat([trans_2, down_3], dim=1)
        up_2 = self.up_2(concat_2)
        trans_3 = self.trans_3(up_2)
        concat_3 = torch.cat([trans_3, down_2], dim=1)
        up_3 = self.up_3(concat_3)
        trans_4 = self.trans_4(up_3)
        concat_4 = torch.cat([trans_4, down_1], dim=1)
        up_4 = self.up_4(concat_4)
        out = self.out(up_4)
        return out
```

* forward에서 concat 연산을 통해 skip-connection 구현
	* 스킵 커넥션을 통해 원본 이미지의 위치 정보를 전달받으므로 비교적 정확한 위치 복원 가능
	* 앞의 ResNet과 비슷한 연산 (ResNet에서는 텐서 간의 합으로 구현)