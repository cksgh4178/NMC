# 합성곱 신경망
CNN 기본 모델 및 유명한 모델들
CNN의 기본 원리에 대한 설명은 합성곱 신경망 정리.md 설명이 잘 되어있으니 그걸 참고



* 보통 모델과 유틸함수, train, test 함수를 별도의 모듈로 작성하는 경우가 많음
* 앞으로의 모델들은 코드가 길고 DataLoader 등 데이터 준비 과정도 코드가 길기 때문에 여러 개의 모듈로 나눠서 작성할 예정

#### model.py

```python
import torch.nn as nn

class cnn(nn.Module):
    def __init__(self, batch_size):
        super(cnn, self).__init__()
        self.batch_size = batch_size
        
        self.conv_layer = nn.Sequential(
            nn.Conv2d(1, 16, 5),
            nn.ReLU(),
            nn.Conv2d(16, 32, 5),
            nn.ReLU(),
            nn.MaxPool2d(2,2),
            nn.Conv2d(32, 64, 5),
            nn.ReLU(),
            nn.MaxPool2d(2,2)
            )
        
        self.fc_layer = nn.Sequential(
            nn.Linear(64*3*3, 100),
            nn.ReLU(),
            nn.Linear(100, 10)
            )
        
    def forward(self, x):
        out = self.conv_layer(x)
        out = out.view(self.batch_size, -1)
        out = self.fc_layer(out)
        return out
```
* 보통 모델은 별도의 모듈로 작성 (import 후에 바로 사용 가능, 수정 및 작성하기 편함)
* 별도의 모듈로 작성할 때 batch_size나 layer 개수 등을 인수로 받아 클래스 생성할 때 클래스 내부 변수로 주로 둠
```python
def __init__(self, batch_size):
	self.batch_size = batch_size
```
<br>

#### data_utils.py
```python
import torchvision.datasets as dset
import torchvision.transforms as transforms
from torch.utils.data import DataLoader

def dataloaders(download, batch_size, num_workers, drop_last):
      mnist_train = dset.MNIST('../data', train=True, transform=transforms.ToTensor(),
                               target_transform=None, download=download)
      mnist_test = dset.MNIST('../data', train=False, transform=transforms.ToTensor(),
                              target_transform=None, download=download)
      
      train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True,
                                num_workers=num_workers, drop_last=drop_last)
      test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True,
                                num_workers=num_workers, drop_last=drop_last)
      return train_loader, test_loader
```
* MNIST 데이터 다운로드 및 DataLoader로 변환 후 return
<br>

#### launch.py

```python
import os
import sys
import torch
import torch.nn as nn
import wandb

base_dir = os.path.dirname(os.getcwd())
sys.path.append(base_dir)
from src.data_utils import dataloaders
from src.model import cnn


# settings
batch_size = 256
learning_rate = 1e-3
epoch = 10
download = False
num_workers = 0
drop_last = True
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')

# wandb settings
wandb.init(project="project", entity="id")
wandb.run.name = 'cnn'

# model
model = cnn(batch_size).to(device)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

train_loader, test_loader = dataloaders(download, batch_size, num_workers, drop_last)

# trian
for i in range(epoch):
    loss = 0.
    for _, (x, y) in enumerate(train_loader):
        x, y = x.to(device), y.to(device)
        pred = model(x)
        optimizer.zero_grad()
        err = criterion(pred, y)
        loss += err.item()
        err.backward()
        optimizer.step()
    
    loss /= len(train_loader)
    wandb.log({'train_loss':loss})
    
# test
total = 0.
correct = 0.
with torch.no_grad():
    for _, (x, y) in enumerate(test_loader):
        x, y = x.to(device), y.to(device)
        pred = model(x)
        _, index = torch.max(pred, 1)
        total += y.size(0)
        correct += (index == y).sum().float()
    wandb.log({'acc':100*correct / total})
```

* os, sys를 사용해 spyder 상에서 코드를 실행하는 것이 아니라 prompt를 이용한 실행 (sys를 사용해 기본 경로를 설정해줘야 설정한 폴더 구조에 맞는 import 가능)
* num_worker를 설정할 경우 prompt 상에서 동작 안함 (추후 업데이트 예정)
```python
model = cnn().to(device)
x, y = x.to(device), y.to(device)
```
* GPU 를 활용하려면 계산하려는 모든 텐서가 전부 GPU상에 올려져있어야함
* 모델과 변수 둘 중 하나라도 디바이스가 다르면 오류가 남
