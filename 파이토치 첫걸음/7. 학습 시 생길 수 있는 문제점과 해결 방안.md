# 학습 시 생길 수 있는 문제점과 해결 방안
<br>

## 오버피팅과 언더피팅
<center><img src="https://imgs.developpaper.com/imgs/56960871-cba15bb68bf0d50e_articlex.png" width = "500" height ="300"></center>

* 위 그림에서 두 오차의 차이가 *일반화 차이(Generalization gap)*
* 모델 학습의 최종 목적은 테스트 오차를 줄이는 것이기 때문에 결국 일반화 차이를 줄이는 것이 목표
	* 테스트 오차 - 학습 오차 = 일반화 차이
	* 테스트 오차 = 학습 오차 + 일반화 오차

	|학습 오차와 일반화 차이 관계|학습 오차 큼|학습오차 작음|
	|:-:|:-:|:-:|
	|일반화 차이 큼|언더피팅|오버피팅|
	|일반화 차이 작음|언더피팅|이상적인 학습|

<br>

* 편차와 분산 간의 관계
<center><img src="https://t1.daumcdn.net/thumb/R720x0.fpng/?fname=http://t1.daumcdn.net/brunch/service/user/bhYD/image/eMtDX_-uWf1LoceceMO2ag6-pj0.png" width="500" height ="500"></center>

* 전체 오차가 고정일 때 편차와 분산은 둘 중 하나가 감소하면 하나는 증가하는 구조로 이루어져 있음
  * 편차 : 예측값과 실제값의 차이의 기댓값
  * 분산 : 예측을 여러 번 했을 때 예측값의 분산
* 언더 피팅은 편차는 높지만 분산은 작음
* 오버 피팅은 편차는 작지만 분산은 큼
<br>

## 정형화 (Regularization)
* 정형화란 어떤 제약 조건을 추가적으로 걸어주어 오버피팅을 해결하려는 기법
* 이 제약 조건은 주로 손실함수에 추가됨
* L1 정형화와 L2 정형화가 대표적 (각각 L1손실과 L2손실에 조건을 추가)
* 정형화가 안될수록 모델 파라미터들이 들쭉날쭉하게 조정됨
* 수식
	* L1 정형화 : ![l1](https://user-images.githubusercontent.com/23060537/143209812-60925fa6-f331-4482-a40b-3f690dc2320e.png)
	* L2 정형화 : ![l2](https://user-images.githubusercontent.com/23060537/143210108-b45f27dc-a7e6-4391-ad11-847cdb4a2470.png)
    * 위 수식을 L1 또는 L2 페널티(penalty) 라고 함
* L2 정형화는 수식적인 해를 구할 수 있지만 w의 값이 완전히 0이 되는 경우가 L1보다 적음
* L1 정형화는 0을 만들 수 있으므로 어떤 특성이 오차를 줄이는데 영향이 없는 지를 알 수 있음 => L1 정형화를 *특성 선택(Feature Selection)* 이라고도 함
* 학습 시 L1, L2 정형화 둘 중 하나를 선택할 수도 있고, 둘 다 적용할 수도 있음 (일래스틱 정형화(elastic regularization))
* **파이토치에서는 optim의 최적화 함수에서 L2 정형화만 지정가능(L1 정형화를 하려면 손실함수에 명시적으로 추가해야함)**
<br>
## 드롭아웃 (dropout)
* 드롭아웃은 특정 뉴런의 확률 p를 0으로 바꿔 해당 뉴런을 죽이는 방법
* 모델 수용력이 p만큼 각 층의 수용력이 줄어드므로 전체 모델의 수용력이 감소됨 (과적합 방지)
	* 드롭아웃은 수용력이 낮은 모델들의 앙상블 개념으로 볼 수도 있음
* 테스트 시에 모든 뉴런의 값을 전달하게 되면 기존 값들보다 더 큰 값이 전달되어 문제가 생길 수도 있음
	* 이 차이를 맞춰주기 위해 테스트 시에는 드롭 확률을 곱해줌
<br>

## 데이터 증강 (data augmentation)
* 데이터 증강은 말 그대로 데이터를 늘리는 방법
	* 주로 이미지에서 사용 (이미지를 회전하거나 뒤집거나 상하 반전 등)
	<br>
## 초기화
* 모델 파라미터 초깃값에 따라 학습의 성능이 좌우되므로 모델 초깃값 설정이 중요
	* Xavier Gloort, Kaiming He 등이 유명한 초깃값 설정 방법
* Xavier Gloort (Xavier 초기화) : 가중치의 초깃값을 ![xavier](https://user-images.githubusercontent.com/23060537/143210137-0da5cd96-dcb0-4188-97ef-dfd7455c83c6.png)에서 설정
* Kaiming He 초기화 : 가중치의 초깃값을 ![Kamining](https://user-images.githubusercontent.com/23060537/143210159-65ef9a77-85bb-4848-aa58-836503c08ac4.png) 에서 설정
* 활성화 함수로 시그모이드나 하이퍼볼릭 탄젠트를 사용하는 경우에는 주로 Xavier, 렐루를 사용하는 경우에는 Kaiming
<br>

## 학습률
* 학습률은 딥러닝 모델 학습에 큰 영향을 미치는 매우 중요한 하이퍼 파라미터
* 보통 높은 학습률로 시작해서 epoch에 따라 낮은 학습률로 변경하는 방식을 주로 사용하지만 배치 사이즈가 더 중요하다는 연구 결과도 있음
* 파이토치의 optim 내에 lr_scheduler 로 구현되어 있음
<br>
## 정규화
* 데이터의 분포가 다른 경우 제대로 된 성능이 안나올 수도 있음
* 정규화로는 평균과 분산을 이용하는 표준화(standadization)와 최소, 최대값을 이용하는 최소극대화가 있음
	* 최소극대화의 경우 평균적 범위를 넘어서는 너무 작거나 너무 큰 이상치가 있는 경우 오히려 학습에 방해가 되기도 함
* 정규화를 하면 손실 그래프가 원형에 가까운 형태를 가지기 때문에 불필요한 업데이트가 적고 더 큰 학습률을 적용할 수 있음
<center><img src="http://jsideas.net/assets/img/20180128.png" width="700" height="350"></center>
<br>

## 배치 정규화
* 모델에 들어오는 데이터의 공분량 변화도 중요하지만 모델 내에서도 하나의 입력층에 여러 범위의 데이터가 들어오는 내부 공변량 변화도 중요
* 배치 정규화는 이런 문제를 해결하기 위해 한 번에 입력으로 들어오는 배치 단위로 정규화하는 것을 의미
* 입력된 배치에 대해 평균과 분산을 구하고 정규화된 데이터를 스케일 및 시프트 하여 다음 레이어에 일정한 범위의 값들만 전달
<center><img src="https://gaussian37.github.io/assets/img/dl/concept/batchnorm/batchnorm.png" width = "400" height ="400"></center>

* 학습 시 배치 단위의 평균과 분산들을 차례대로 받아 이동평균과 이동분산을 저장했다가 테스트 할 때 평균과 분산으로 정규화
* 배치 정규화는 보통 합성곱 연산이나 선형 변환 연산 사이에 들어감
* 최근에는 데이터 하나 당 정규화를 하는 인스턴스 정규화나 은닉층 가중치 정규화 등이 소개됨
<br>

## 경사하강법 변형
* 파이토치 optim 내에 구현되어 있는 다양한 최적화 함수에 대한 설명
