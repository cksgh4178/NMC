# 선형 회귀

```python
import torch
import torch.nn as nn
import pandas as pd

# data prepare
x = nn.init.uniform_(torch.Tensor(100, 1), -10, 10)
noise = nn.init.normal_(torch.Tensor(100,1), std = 1)
y = 2*x + 3 + noise

# plotting
plot = pd.DataFrame(columns=['x', 'y'], data = torch.cat((x,y),dim=1).numpy())
plot.sort_values(by = 'x', inplace = True)
plot.plot.scatter(x = 'x', y = 'y')
```
* x는 [-10, 10] 사이에서 균등하게 총 100개 생성
* noise는 표준편차가 1인 정규분포
* y = 2x+3에 noise가 추가된 값

![Figure 2021-11-17 173729](https://user-images.githubusercontent.com/23060537/142168099-75b79547-91f2-43c9-908a-9e3d4d8d9f58.png)

```python
# model config
model  = nn.Linear(1, 1)

# training config
learning_rate = 0.01
epoch = 500

# train
criterion = nn.L1Loss() # MAE 계산식과 동일
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

model.train()
losses = list()
for i in range(epoch):
    pred = model(x)
    optimizer.zero_grad()
    loss = criterion(pred, y)
    losses.append(loss.item())
    loss.backward()
    optimizer.step()

# loss 변화 그래프
losses = pd.Series(losses)
losses.plot()

# 결과 비교
plot = pd.DataFrame(columns=['x', 'real','pred'], data = torch.cat((x,y, pred.detach()),dim=1).numpy())
plot.sort_values(by = 'x', inplace = True)
plot.plot(x = 'x', ylabel = 'y')
```

loss 변화
<center>![Figure 2021-11-17 173958](https://user-images.githubusercontent.com/23060537/142168157-16ee4909-1aa0-4aea-bbd6-12eb766f5c69.png)</center>

실제값 - 예측값
<center>![Figure 2021-11-17 174823](https://user-images.githubusercontent.com/23060537/142168174-0b633004-b3ad-4d8e-b10d-71a7d2eb5a85.png)</center>

선형 회귀 모델 직접 구성
```python
W = torch.zeros([1], requires_grad=True)
b = torch.zeros([1], requires_grad=True)
```
wandb 로 학습 과정 추적
```python
import wandb
wandb.init(project="your project name", entity="your ID") 
wandb.run.name = 'linear_regression'
```
학습 세팅 및 학습 과정
```python
epoch = 500
learning_rate = 0.01
criterion = nn.L1Loss()
optimizer = torch.optim.SGD([W,b], lr=learning_rate)

for i in range(epoch):
  pred = x.matmul(W) + b
  optimizer.zero_grad()
  loss = criterion(pred,y.squeeze())
  loss.backward()
  optimizer.step()
  
  wandb.log({
      'loss':loss.item(),
      'W':W.detach().item(),
      'b':b.detach().item()})
```